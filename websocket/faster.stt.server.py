# websocket/faster.stt.server.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streaming Î¼-law STT server using Faster-Whisper (CUDA) + websockets.
# Listens on ws://0.0.0.0:8002 by default.  One connection per call stream.

import asyncio, json, os, struct, wave, tempfile, audioop
import numpy as np
import websockets
from faster_whisper import WhisperModel
import torch

# â”€â”€â”€â”€â”€ config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HOST               = "0.0.0.0"
PORT               = 8002
MODEL_NAME         = "large-v3"            # whisper-large-v3 (~1 GB CT2)
DEVICE             = "cuda"                # "cuda" or "cpu"
COMPUTE_TYPE       = "float16"             # "float16" (GPU)  |  "int8_float16" (CPU)
FRAME_BYTES        = 160                   # 20 ms Î¼-law @ 8 kHz mono
FRAMES_PER_CHUNK   = 100                   # â†’ 2.0 s batches; adjust to taste
LANGUAGE           = "en"                  # force language (or None for auto)

# â”€â”€â”€â”€â”€ load model once at start-up â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"Loading Whisper {MODEL_NAME} on {DEVICE}â€¦")
model = WhisperModel(MODEL_NAME, device=DEVICE, compute_type=COMPUTE_TYPE)
print("Whisper ready âœ…")

# â”€â”€â”€â”€â”€ helper: Î¼-law bytes â†’ float32 numpy @ 16 kHz mono â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def mulaw_to_f32_16k(raw_ulaw: bytes) -> np.ndarray:
    # Î¼-law 8 kHz â†’ linear PCM s16 (little-endian)
    pcm8 = audioop.ulaw2lin(raw_ulaw, 2)             # 8 kHz, 16-bit
    # resample 8 kHz â†’ 16 kHz
    pcm16, _ = audioop.ratecv(pcm8, 2, 1, 8000, 16000, None)
    # int16 â†’ float32 in -1.0 â€¦ 1.0
    return np.frombuffer(pcm16, dtype="<i2").astype(np.float32) / 32768.0

# â”€â”€â”€â”€â”€ per-socket handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def handler(ws: websockets.WebSocketServerProtocol):
    print("ðŸ†•  peer connected:", ws.remote_address)
    frame_buf = bytearray()

    async for message in ws:
        # Twilio sends raw binary frames â€“ no JSON wrapper
        if isinstance(message, (bytes, bytearray)):
            frame_buf.extend(message)

            if len(frame_buf) >= FRAME_BYTES * FRAMES_PER_CHUNK:
                await transcribe_and_send(frame_buf, ws)
                frame_buf.clear()

        # (If you ever send control JSON from Node, handle it here)

    # flush any leftover audio
    if frame_buf:
        await transcribe_and_send(frame_buf, ws)
    print("ðŸ‘‹  peer disconnected:", ws.remote_address)

# â”€â”€â”€â”€â”€ STT + emit JSON back to caller â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def transcribe_and_send(buf: bytearray, ws):
    audio_f32 = mulaw_to_f32_16k(buf)

    segments, _ = model.transcribe(
        audio_f32,
        language=LANGUAGE,
        vad_filter=True, vad_parameters=dict(min_silence_duration_ms=200))

    text = " ".join(seg.text.strip() for seg in segments).strip()
    if text:
        payload = json.dumps({"is_final": True, "text": text})
        await ws.send(payload)
        print("ðŸ”Š  â†’", text)

# â”€â”€â”€â”€â”€ main entry - start server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def main():
    async with websockets.serve(handler, HOST, PORT, max_size=2**20):
        print(f"ðŸ—£  STT WebSocket on ws://{HOST}:{PORT}")
        await asyncio.Future()  # run forever

if __name__ == "__main__":
    if DEVICE == "cuda" and not torch.cuda.is_available():
        raise RuntimeError("CUDA requested but not available.")
    asyncio.run(main())
